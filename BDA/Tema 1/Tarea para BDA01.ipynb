{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea Online BDA01\n",
    "\n",
    "## Nombre del alumno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta tarea es que despliegues el entorno de Hadoop que vamos a usar en el resto de unidades. Para ello debes realizar paso a paso el proceso que se explicó en la guía práctica. Después podrás entregar esta plantilla sustituyendo las celdas de markdown o de código por lo que se pide. Lee detenidamente el enunciado ya que a veces se pedirá que incluyas una imagen, otras texto y en algunas código ejecutable. El libro de Jupyter que entregues deberá tener las celdas completamente ejecutadas en orden secuencial, no debe tener errores y corresponderá con la ejecución real que obtuviste al ejecutarlo en el contenedor `namenode` de tu equipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienza copiando este libro, `Tarea\\ para\\ BDA01.ipynb`, en el servidor de Jupyter. Recuerda que el directorio `notebooks` del anfitrión está montado en el directorio `/media/notebooks` del contenedor llamado `namenode` de `docker`. Cualquier cosa que sitúes en este directorio (o en un subdirectorio) será accesible tanto por el anfitrión como por el contenedor.\n",
    "\n",
    "Si el anterior párrafo no tiene sentido para ti, probablemente debas revisar de nuevo la guía práctica antes de seguir con la tarea práctica.\n",
    "\n",
    "Es posible que al intentar copiar el libro `Tarea\\ para\\ BDA01.ipynb` tengas un problema de permisos. Si esto sucede es porque el contenedor de `docker` está asignando la propiedad de los archivos al usuario `root`. Este problema se puede solucionar de distintas formas:\n",
    "\n",
    "* Asignando permisos de escritura a todos los usuarios (comando `chmod` en Linux).\n",
    "* Usando el comando `sudo`.\n",
    "* Copiando el archivo desde el anfitrión al contenedor con el comando `cp` de `docker`:\n",
    "```bash\n",
    "docker cp Tarea\\ para\\ BDA01.ipynb namenode:/media/notebooks/unidad\\ 1/\n",
    "```\n",
    "* Si todo lo anterior no funciona, pregunta en el foro.\n",
    "\n",
    "Una vez tengas la plantilla de la tarea en el servidor de Jupyter, comienza a completar las celdas que empiecen por \"TODO\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Instala `docker` y `docker-compose` en tu equipo y copia una imagen en la que se muestre el resultado de ejecutar en el anfitrión las siguientes instrucciones:\n",
    "\n",
    "```bash\n",
    "docker --version\n",
    "docker-compose --version\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ejercicio1](./img/ejercicio1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Levanta el entorno de Hadoop con `docker-compose` haciendo lo que se indica en la guía práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ejercicio2](./img/ejercicio2.png)\n",
    "\n",
    "![Ejercicio2](./img/ejercicio2.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Muestra en Jupyter el contenido del directorio en el que estás ejecutando este libro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ejercicio3](./img/ejercicio3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.- Realiza todo el proceso que se describe en la guía práctica para obtener el número de palabras que contiene *El Quijote*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-18 23:53:00--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘media/notebooks/unidad1/2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M   189KB/s    in 20s     \n",
      "\n",
      "2023-12-18 23:53:21 (107 KB/s) - ‘media/notebooks/unidad1/2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -P media/notebooks/unidad1/ https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ejercicio4](./img/ejercicio4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El Quijote tiene 383640 palabras.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('media/notebooks/unidad1/2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "\n",
    "book = lines[head:-tail]\n",
    "book = \"\".join(book)\n",
    "book = book.lower()\n",
    "\n",
    "import re\n",
    "\n",
    "book = re.split('\\W', book)\n",
    "book = list(filter(lambda word: len(word), book))\n",
    "\n",
    "\"El Quijote tiene {} palabras.\".format(len(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.- Crea un proceso en Python que muestre las 10 palabras que más se utilizan en *El Quijote* junto con el número de veces que aparecen en el libro.\n",
    "\n",
    "Los ejercicios anteriores pretendían simplemente que replicaras el trabajo que se realizó en la guía práctica. Este ejercicio tiene como objetivo que empieces a crear tus primeros programas en Python para que te resulten más fáciles las próximas prácticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('que', 20742), ('de', 18409), ('y', 18251), ('la', 10489), ('a', 9928), ('el', 8261), ('en', 8259), ('no', 6341), ('los', 4768), ('se', 4752)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ruta_archivo = 'media/notebooks/unidad1/2000-0.txt'\n",
    "\n",
    "with open('media/notebooks/unidad1/2000-0.txt') as archivo:\n",
    "    contenido = archivo.read()\n",
    "\n",
    "signos = \".,:;-[]{}()'\\\"@/¿?¡!\"\n",
    "a_espacios = str.maketrans(signos, \" \"*len(signos))\n",
    "palabras = str.translate(contenido, a_espacios).lower().split()\n",
    "contador = Counter(palabras)\n",
    "print(contador.most_common(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frecuencias como diccionario: Counter({'las': 3, 'el': 2, 'los': 1, 'la': 1})\n",
      "freciencias como tuplas: dict_items([('el', 2), ('las', 3), ('los', 1), ('la', 1)])\n"
     ]
    }
   ],
   "source": [
    "#Este es un ejemplo del uso de counter\n",
    "from collections import Counter\n",
    "\n",
    "freqs = Counter(['el', 'las', 'los', 'el', 'las', 'la', 'las'])\n",
    "print(\"frecuencias como diccionario:\", freqs)\n",
    "print(\"freciencias como tuplas:\", freqs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.- (OPCIONAL) Los dos últimos ejercicios son tareas habituales en el mundo de Big Data, concretamente pertenecen a un área denominada Procesamiento de Lenguaje Natural (NLP). Sin embargo, la solución que se pidió es trivial y tiene bastantes problemas para ser aplicada en un entorno profesional:\n",
    "\n",
    "* Por un lado no admite trabajar con un gran volumen de datos. De esto nos ocuparemos en próximas prácticas ya que este es el problema esencial al que tratamos de dar solución en este módulo.\n",
    "* Por otro, tanto la división en palabras (ejercicio 4), como el cálculo de la frecuencia de aparición (ejercicio 5), hacen suposiciones poco realistas: Para la división en palabras hemos supuesto que cualquier carácter no alfanumérico sirve para separar dos palabras, pero esto no es siempre cierto. Por ejemplo, ¿cuántas palabras hay en O.T.A.N.?, ¿cinco o una? Además, para contar las palabras más frecuentes no hemos tenido en cuenta que una misma palabra se puede presentar en diferentes formas. Por ejemplo: \"la\" y \"las\", ¿son la misma palabra o dos palabras diferentes?, ¿y \"fue\" e \"irá\"? La respuesta a esta pregunta no es unívoca y dependerá del objetivo del estudio.\n",
    "\n",
    "Las herramientas de NLP nos permiten tanto dividir un texto en palabras (\"tokenization\"), como \n",
    "obtener el [lema](https://es.wikipedia.org/wiki/Lema_(ling%C3%BC%C3%ADstica)#:~:text=El%20lema%20es%20la%20unidad,o%20no%2C%20de%20un%20lema.) de una palabra (\"lemmatisation\"). En este ejercicio se pretende que resuelvas los ejercicios 4 y 5 usando [`spaCy`](https://spacy.io/), que es una herramienta NLP popular en el mundo Python. Dado que se trata de una actividad voluntaria y para hacerlo más interesante, no se va a dar ninguna indicación de cómo realizar el proceso. Incluso aunque no llegues a resolver el ejercicio, es interesante que consultes la solución cuando esta se publique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Utiliza `spaCy` para resolver los ejercicios 3 y 4. Sustituye esta celda por la solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 3.2 MB/s eta 0:00:01     |███████████████▌                | 3.2 MB 3.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 996 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
      "\u001b[K     |████████████████████████████████| 494 kB 569 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (2.10.1)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 182 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[K     |████████████████████████████████| 381 kB 405 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 5.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.15.0; python_version < \"3.9\"\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (934 kB)\n",
      "\u001b[K     |████████████████████████████████| 934 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 398 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic-core==2.14.5\n",
      "  Downloading pydantic_core-2.14.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<0.2.0,>=0.0.4\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 1.5 MB/s eta 0:00:01    |██████████████                  | 4.5 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: wasabi, idna, charset-normalizer, urllib3, certifi, requests, catalogue, srsly, murmurhash, cymem, preshed, tqdm, packaging, langcodes, typing-extensions, pydantic-core, annotated-types, pydantic, click, typer, smart-open, cloudpathlib, confection, weasel, numpy, blis, thinc, spacy-loggers, spacy-legacy, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 idna-3.6 langcodes-3.3.0 murmurhash-1.0.10 numpy-1.24.4 packaging-23.2 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 requests-2.31.0 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 tqdm-4.66.1 typer-0.9.0 typing-extensions-4.9.0 urllib3-2.1.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.9 MB 626 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (45.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.24.4)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.14.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: confection<0.2.0,>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Para instalar spaCy en el contenedor de Jupyter, ejecuta la siguiente instrucción\n",
    "\n",
    "! pip3 install spacy\n",
    "\n",
    "# Para descargar el tokenizer de spaCy en español\n",
    "\n",
    "! python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4\n",
    "import spacy\n",
    "\n",
    "ruta_archivo = 'media/notebooks/unidad1/2000-0.txt'  # Reemplaza con la ruta correcta de tu archivo\n",
    "\n",
    "# Cargar el modelo de lenguaje de spaCy (por ejemplo, el modelo en español 'es_core_news_sm')\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Leer el archivo y procesarlo con spaCy\n",
    "with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "\n",
    "book = lines[head:-tail]\n",
    "book = \"\".join(book)\n",
    "book = book.lower()\n",
    "\n",
    "# Procesar el contenido del libro con spaCy\n",
    "doc = nlp(book)\n",
    "\n",
    "# Filtrar palabras y contar su frecuencia\n",
    "palabras = [token.text for token in doc if token.is_alpha]\n",
    "\n",
    "print(\"El Quijote tiene {} palabras.\".format(len(palabras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "ruta_archivo = 'media/notebooks/unidad1/2000-0.txt'  # Reemplaza con la ruta correcta de tu archivo\n",
    "\n",
    "# Cargar el modelo de lenguaje de spaCy (por ejemplo, el modelo en español 'es_core_news_sm')\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()  # Convertir a minúsculas\n",
    "    texto = re.sub(r'[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ\\s]', '', texto)  # Mantener solo letras y espacios\n",
    "    return texto\n",
    "\n",
    "# Leer el archivo\n",
    "with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "    contenido = archivo.read()\n",
    "\n",
    "# Limpiar el texto y dividir en fragmentos más pequeños\n",
    "contenido_limpio = limpiar_texto(contenido)\n",
    "fragmentos = [contenido_limpio[i:i+100000] for i in range(0, len(contenido_limpio), 100000)]\n",
    "\n",
    "# Procesar los fragmentos del archivo con spaCy\n",
    "contador = Counter()\n",
    "\n",
    "for fragmento in fragmentos:\n",
    "    doc = nlp(fragmento)\n",
    "    palabras = [token.text for token in doc if token.is_alpha]\n",
    "    contador.update(palabras)\n",
    "\n",
    "# Mostrar las 10 palabras más comunes\n",
    "print(contador.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
