{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea online BDA02\n",
    "\n",
    "# Nombre del alumno: Victoria Jiménez Martín\n",
    "\n",
    "En esta tarea deberás completar las celdas que están incompletas. Se muestra el resultado esperado de la ejecución. Se trata de que implementes un proceso MapReduce que produzca ese resultado. Puedes implementar el proceso MapReduce con el lenguaje y librería que prefieras (`Bash`, Python, `mrjob` ...). Los datos de entrada del proceso son meros ejemplos y el proceso que implementes debería funcionar con esos y cualquier otro fichero de entrada que tenga la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Partiendo del fichero de `notas.txt`, calcula la nota más alta obtenida por cada alumno con un proceso MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, que si tenemos el fichero de notas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.txt\n",
    "pedro 6 7\n",
    "luis 0 4\n",
    "ana 7\n",
    "pedro 8 1 3\n",
    "ana 5 6 7\n",
    "ana 10\n",
    "luis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera obtener el siguiente resultado:\n",
    "\n",
    "![solución 1](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MaxGrades(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        student, *grades = line.split()\n",
    "        grades = [int(grade) for grade in grades]\n",
    "        yield student, max(grades)\n",
    "\n",
    "    def reducer(self, student, grades):\n",
    "        yield student, max(grades)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MaxGrades.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x notas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/notas.root.20240117.173229.083758\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2317807799491329889/] [] /tmp/streamjob2711160104362418343.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0005/\n",
      "  Running job: job_1705510300565_0005\n",
      "  Job job_1705510300565_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=834321\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=376\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=28\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4918272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1925120\n",
      "\t\tTotal time spent by all map tasks (ms)=4803\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4803\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1880\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1880\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4803\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1880\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1420\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tInput split bytes=284\n",
      "\t\tMap input records=7\n",
      "\t\tMap output bytes=63\n",
      "\t\tMap output materialized bytes=89\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=343412736\n",
      "\t\tPeak Map Virtual memory (bytes)=2549915648\n",
      "\t\tPeak Reduce Physical memory (bytes)=202092544\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2554277888\n",
      "\t\tPhysical memory (bytes) snapshot=885264384\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=89\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=891813888\n",
      "\t\tVirtual memory (bytes) snapshot=7654047744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output...\n",
      "\"ana\"\t10\n",
      "\"luis\"\t4\n",
      "\"pedro\"\t8\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758...\n",
      "Removing temp directory /tmp/notas.root.20240117.173229.083758...\n"
     ]
    }
   ],
   "source": [
    "! python3 notas.py -r hadoop notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Usando un proceso MapReduce muestra las 10 palabras más utilizadas en `El Quijote`.\n",
    "\n",
    "Lo primero será descargar El Quijote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-07 23:29:34--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M   761KB/s    in 2.9s    \n",
      "\n",
      "2024-01-07 23:29:38 (761 KB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O '2000-0.txt' https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la primera práctica, eliminamos aquellas líneas que son metadata y no forman parte de la obra. Sobrescribimos el fichero sin esas líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing palabrasQuijote.py\n"
     ]
    }
   ],
   "source": [
    "with open('2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "book = lines[head:-tail]\n",
    "\n",
    "with open('2000-0.txt', 'w') as f:\n",
    "    for line in book:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debería ser el mismo que el que obtuvimos en la primera práctica.\n",
    "\n",
    "![solución 2](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting palabrasQuijote.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile palabrasQuijote.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MostUsedWords(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_words(self, _, word_count_pairs):\n",
    "        top_words = sorted(word_count_pairs, reverse=True)[:10]\n",
    "        for count, word in top_words:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostUsedWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x palabrasQuijote.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/palabrasQuijote.root.20240117.173636.260181\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2934571464954495916/] [] /tmp/streamjob1139812660036353079.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0007/\n",
      "  Running job: job_1705510300565_0007\n",
      "  Job job_1705510300565_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2227135\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=527951\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4585617\n",
      "\t\tFILE: Number of bytes written=10005884\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2227441\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=527951\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7640064\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3682304\n",
      "\t\tTotal time spent by all map tasks (ms)=7461\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7461\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3596\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3596\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7461\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3596\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3490\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=218\n",
      "\t\tInput split bytes=306\n",
      "\t\tMap input records=38062\n",
      "\t\tMap output bytes=3811615\n",
      "\t\tMap output materialized bytes=4585623\n",
      "\t\tMap output records=386998\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=342708224\n",
      "\t\tPeak Map Virtual memory (bytes)=2549731328\n",
      "\t\tPeak Reduce Physical memory (bytes)=198610944\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553376768\n",
      "\t\tPhysical memory (bytes) snapshot=849338368\n",
      "\t\tReduce input groups=23837\n",
      "\t\tReduce input records=386998\n",
      "\t\tReduce output records=23837\n",
      "\t\tReduce shuffle bytes=4585623\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=773996\n",
      "\t\tTotal committed heap usage (bytes)=860356608\n",
      "\t\tVirtual memory (bytes) snapshot=7647924224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar935398888758465113/] [] /tmp/streamjob8022755616211004440.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0008\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0008\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0008/\n",
      "  Running job: job_1705510300565_0008\n",
      "  Job job_1705510300565_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=532047\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=104\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=575631\n",
      "\t\tFILE: Number of bytes written=1985630\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=532375\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=104\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4273152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2249728\n",
      "\t\tTotal time spent by all map tasks (ms)=4173\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4173\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2197\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2197\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4173\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2197\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=191\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=23837\n",
      "\t\tMap output bytes=527951\n",
      "\t\tMap output materialized bytes=575637\n",
      "\t\tMap output records=23837\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=338903040\n",
      "\t\tPeak Map Virtual memory (bytes)=2556891136\n",
      "\t\tPeak Reduce Physical memory (bytes)=204029952\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553331712\n",
      "\t\tPhysical memory (bytes) snapshot=852463616\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=23837\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=575637\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=47674\n",
      "\t\tTotal committed heap usage (bytes)=860356608\n",
      "\t\tVirtual memory (bytes) snapshot=7658295296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output...\n",
      "\"que\"\t20769\n",
      "\"de\"\t18410\n",
      "\"y\"\t18266\n",
      "\"la\"\t10491\n",
      "\"a\"\t9932\n",
      "\"en\"\t8281\n",
      "\"el\"\t8265\n",
      "\"no\"\t6344\n",
      "\"los\"\t4769\n",
      "\"se\"\t4752\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181...\n",
      "Removing temp directory /tmp/palabrasQuijote.root.20240117.173636.260181...\n"
     ]
    }
   ],
   "source": [
    "! python3 palabrasQuijote.py -r hadoop 2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Muestra la clasificación de temporada 2021/2022 de La Liga pero únicamente de los puntos obtenidos como visitante.\n",
    "\n",
    "En [esta Web](https://resultados.as.com/resultados/futbol/primera/2021_2022/clasificacion/) puedes consultar cuántos puntos obtuvo cada equipo fuera de casa.\n",
    "\n",
    "Empezamos descargando el fichero de resultados de la temporada 2021/2022 y renombrándolo a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-07 23:47:25--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K   722KB/s    in 0.2s    \n",
      "\n",
      "2024-01-07 23:47:26 (722 KB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera este resultado:\n",
    "\n",
    "![solución 3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificacion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificacion.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "\n",
    "class VisitorPointsMRJob(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        row = dict(zip(['Div', 'Date', 'Time','HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'], next(csv.reader([line]))))\n",
    "\n",
    "        visitor_team = row['AwayTeam']\n",
    "        points = 0\n",
    "        if row['FTR'] == 'A':\n",
    "            points = 3\n",
    "        elif row['FTR'] == 'D':\n",
    "            points = 1\n",
    "\n",
    "        yield visitor_team, points\n",
    "\n",
    "    def reducer(self, team, points):\n",
    "        yield team, sum(points)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    VisitorPointsMRJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x clasificacion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/clasificacion.root.20240117.180628.915861\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4181290725929563182/] [] /tmp/streamjob6530032604275923716.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0014\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0014\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0014\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0014/\n",
      "  Running job: job_1705510300565_0014\n",
      "  Job job_1705510300565_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0014 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=281\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5531\n",
      "\t\tFILE: Number of bytes written=845604\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176580\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=281\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3941376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1817600\n",
      "\t\tTotal time spent by all map tasks (ms)=3849\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3849\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1775\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1775\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3849\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1775\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=310\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=4763\n",
      "\t\tMap output materialized bytes=5537\n",
      "\t\tMap output records=381\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=338718720\n",
      "\t\tPeak Map Virtual memory (bytes)=2548281344\n",
      "\t\tPeak Reduce Physical memory (bytes)=196759552\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553225216\n",
      "\t\tPhysical memory (bytes) snapshot=835440640\n",
      "\t\tReduce input groups=21\n",
      "\t\tReduce input records=381\n",
      "\t\tReduce output records=21\n",
      "\t\tReduce shuffle bytes=5537\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=762\n",
      "\t\tTotal committed heap usage (bytes)=856162304\n",
      "\t\tVirtual memory (bytes) snapshot=7646322688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output...\n",
      "\"Alaves\"\t6\n",
      "\"Ath Bilbao\"\t21\n",
      "\"Ath Madrid\"\t30\n",
      "\"AwayTeam\"\t0\n",
      "\"Barcelona\"\t35\n",
      "\"Betis\"\t33\n",
      "\"Cadiz\"\t21\n",
      "\"Celta\"\t21\n",
      "\"Elche\"\t15\n",
      "\"Espanol\"\t9\n",
      "\"Getafe\"\t11\n",
      "\"Granada\"\t16\n",
      "\"Levante\"\t13\n",
      "\"Mallorca\"\t12\n",
      "\"Osasuna\"\t25\n",
      "\"Real Madrid\"\t42\n",
      "\"Sevilla\"\t28\n",
      "\"Sociedad\"\t27\n",
      "\"Valencia\"\t22\n",
      "\"Vallecano\"\t13\n",
      "\"Villarreal\"\t23\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861...\n",
      "Removing temp directory /tmp/clasificacion.root.20240117.180628.915861...\n"
     ]
    }
   ],
   "source": [
    "! python3 clasificacion.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Muestra la diferencia de goles entre el equipo que más goles ha marcado y el que menos goles ha marcado en la temporada 2021/2022 de La Liga.\n",
    "\n",
    "Se espera que el proceso MapReuce produzca una salida similar a la siguiente:\n",
    "\n",
    "![solución 4](./img/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting goaldifference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile goaldifference.py\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def map_function(row):\n",
    "    \"\"\"Extrae los equipos y los goles marcados.\"\"\"\n",
    "    home_team = row['HomeTeam']\n",
    "    away_team = row['AwayTeam']\n",
    "    home_goals = int(row['FTHG'])  # Goles marcados por el equipo local\n",
    "    away_goals = int(row['FTAG'])  # Goles marcados por el equipo visitante\n",
    "    return [(home_team, home_goals), (away_team, away_goals)]\n",
    "\n",
    "def reduce_function(mapped_data):\n",
    "    \"\"\"Suma los goles marcados para cada equipo.\"\"\"\n",
    "    goals_by_team = defaultdict(int)\n",
    "    for team, goals in mapped_data:\n",
    "        goals_by_team[team] += goals\n",
    "    return goals_by_team\n",
    "\n",
    "# Procesamiento del archivo CSV\n",
    "mapped_data = []\n",
    "\n",
    "with open('laliga2122.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        mapped_data.extend(map_function(row))\n",
    "\n",
    "reduced_data = reduce_function(mapped_data)\n",
    "\n",
    "# Encontrar el equipo con más y menos goles marcados\n",
    "max_goals = max(reduced_data.items(), key=lambda x: x[1])\n",
    "min_goals = min(reduced_data.items(), key=lambda x: x[1])\n",
    "\n",
    "# Calcular la diferencia de goles\n",
    "goal_difference = max_goals[1] - min_goals[1]\n",
    "print(f\"Equipo con más goles: {max_goals[0]} ({max_goals[1]} goles)\")\n",
    "print(f\"Equipo con menos goles: {min_goals[0]} ({min_goals[1]} goles)\")\n",
    "print(f\"Diferencia de goles: {goal_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x goaldifference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipo con más goles: Real Madrid (80 goles)\r\n",
      "Equipo con menos goles: Alaves (31 goles)\r\n",
      "Diferencia de goles: 49\r\n"
     ]
    }
   ],
   "source": [
    "! python3 goaldifference.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Calcula la racha de los últimos cinco partidos de cada equipo en la clasificación final de La Liga en la temporada 2021/2022.\n",
    "\n",
    "[Observa](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) que las últimas columnas de la clasificación muestran cuál ha sido el resultado de los últimos 5 partidos de cada equipo.\n",
    "\n",
    "![clasificacion](./img/clasificacion.png)\n",
    "\n",
    "Se trata de que muestres la clasificación final junto con los resultados de los últimos 5 partidos. Este ejercicio es un poco más difícil y laborioso que los otros. Si usas `mrjob` probablemente te sea útil utilizar [ordenación secundaria por valor](https://mrjob.readthedocs.io/en/latest/job.html#secondary-sort), aunque también se puede resolver sin hacer uso de ella.\n",
    "\n",
    "Se espera este resultado:\n",
    "\n",
    "![solución 5](./img/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificationMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificationMR.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "from collections import defaultdict, deque\n",
    "import csv\n",
    "\n",
    "# Diccionario para almacenar los puntos totales y los resultados de los últimos cinco partidos\n",
    "teams = defaultdict(lambda: {'points': 0, 'last_five': deque(maxlen=5)})\n",
    "\n",
    "# Reemplaza 'path_to_file.csv' con la ruta real al archivo CSV\n",
    "with open('laliga2122.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        home_team = row['HomeTeam']\n",
    "        away_team = row['AwayTeam']\n",
    "        home_goals = int(row['FTHG'])\n",
    "        away_goals = int(row['FTAG'])\n",
    "        match_result = row['FTR']\n",
    "\n",
    "        # Puntos y resultados para el equipo local\n",
    "        if match_result == 'H':  # Victoria local\n",
    "            teams[home_team]['points'] += 3\n",
    "            teams[home_team]['last_five'].append(3)\n",
    "        elif match_result == 'D':  # Empate\n",
    "            teams[home_team]['points'] += 1\n",
    "            teams[home_team]['last_five'].append(1)\n",
    "\n",
    "        # Puntos y resultados para el equipo visitante\n",
    "        if match_result == 'A':  # Victoria visitante\n",
    "            teams[away_team]['points'] += 3\n",
    "            teams[away_team]['last_five'].append(3)\n",
    "        elif match_result == 'D':  # Empate\n",
    "            teams[away_team]['points'] += 1\n",
    "            teams[away_team]['last_five'].append(1)\n",
    "\n",
    "        if match_result == 'H':  # Derrota visitante\n",
    "            teams[away_team]['last_five'].append(0)\n",
    "        elif match_result == 'A':  # Derrota local\n",
    "            teams[home_team]['last_five'].append(0)\n",
    "\n",
    "# Convertir los datos a una lista para poder ordenarlos por puntos\n",
    "team_stats = [(team, data['points'], list(data['last_five'])) for team, data in teams.items()]\n",
    "team_stats.sort(key=lambda x: x[1], reverse=True)  # Ordenar por puntos de manera descendente\n",
    "\n",
    "# Imprimir la información de todos los equipos\n",
    "for team, points, last_five in team_stats:\n",
    "    print(f\"{team}: {points} puntos, Últimos 5 partidos: {last_five}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x clasificationMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid: 86 puntos, Últimos 5 partidos: [3, 0, 3, 1, 1]\r\n",
      "Barcelona: 73 puntos, Últimos 5 partidos: [3, 3, 3, 1, 0]\r\n",
      "Ath Madrid: 71 puntos, Últimos 5 partidos: [0, 3, 3, 1, 3]\r\n",
      "Sevilla: 70 puntos, Últimos 5 partidos: [1, 1, 1, 1, 3]\r\n",
      "Betis: 65 puntos, Últimos 5 partidos: [1, 0, 3, 3, 1]\r\n",
      "Sociedad: 62 puntos, Últimos 5 partidos: [1, 0, 3, 3, 0]\r\n",
      "Villarreal: 59 puntos, Últimos 5 partidos: [0, 1, 3, 0, 3]\r\n",
      "Ath Bilbao: 55 puntos, Últimos 5 partidos: [3, 1, 0, 3, 0]\r\n",
      "Valencia: 48 puntos, Últimos 5 partidos: [1, 1, 0, 1, 3]\r\n",
      "Osasuna: 47 puntos, Últimos 5 partidos: [1, 1, 1, 0, 0]\r\n",
      "Celta: 46 puntos, Últimos 5 partidos: [1, 3, 0, 3, 0]\r\n",
      "Espanol: 42 puntos, Últimos 5 partidos: [0, 1, 0, 1, 1]\r\n",
      "Vallecano: 42 puntos, Últimos 5 partidos: [1, 1, 0, 0, 0]\r\n",
      "Elche: 42 puntos, Últimos 5 partidos: [1, 0, 0, 0, 3]\r\n",
      "Getafe: 39 puntos, Últimos 5 partidos: [1, 1, 1, 1, 0]\r\n",
      "Cadiz: 39 puntos, Últimos 5 partidos: [1, 3, 0, 1, 3]\r\n",
      "Mallorca: 39 puntos, Últimos 5 partidos: [0, 0, 1, 3, 3]\r\n",
      "Granada: 38 puntos, Últimos 5 partidos: [1, 3, 3, 0, 1]\r\n",
      "Levante: 35 puntos, Últimos 5 partidos: [1, 3, 0, 3, 3]\r\n",
      "Alaves: 31 puntos, Últimos 5 partidos: [3, 0, 3, 0, 0]\r\n"
     ]
    }
   ],
   "source": [
    "! python3 clasificationMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
