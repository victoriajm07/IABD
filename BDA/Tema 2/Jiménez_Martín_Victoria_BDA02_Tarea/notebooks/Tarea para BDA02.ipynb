{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea online BDA02\n",
    "\n",
    "# Nombre del alumno: Victoria Jiménez Martín\n",
    "\n",
    "En esta tarea deberás completar las celdas que están incompletas. Se muestra el resultado esperado de la ejecución. Se trata de que implementes un proceso MapReduce que produzca ese resultado. Puedes implementar el proceso MapReduce con el lenguaje y librería que prefieras (`Bash`, Python, `mrjob` ...). Los datos de entrada del proceso son meros ejemplos y el proceso que implementes debería funcionar con esos y cualquier otro fichero de entrada que tenga la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Partiendo del fichero de `notas.txt`, calcula la nota más alta obtenida por cada alumno con un proceso MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, que si tenemos el fichero de notas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.txt\n",
    "pedro 6 7\n",
    "luis 0 4\n",
    "ana 7\n",
    "pedro 8 1 3\n",
    "ana 5 6 7\n",
    "ana 10\n",
    "luis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera obtener el siguiente resultado:\n",
    "\n",
    "![solución 1](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MaxGrades(MRJob):\n",
    "    # Divide cada línea por espacios, signando el primer elemento a student y el resto a grades.\n",
    "    # Luego, envía cada estudiante con su nota más alta.\n",
    "    def mapper(self, _, line):\n",
    "        student, *grades = line.split()\n",
    "        grades = [int(grade) for grade in grades]\n",
    "        yield student, max(grades)\n",
    "        \n",
    "    # Para cada estudiante, toma todas las notas máximas enviadas desde el mapper y calcula la nota máxima global.\n",
    "    def reducer(self, student, grades):\n",
    "        yield student, max(grades)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MaxGrades.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x notas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/notas.root.20240117.173229.083758\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2317807799491329889/] [] /tmp/streamjob2711160104362418343.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0005/\n",
      "  Running job: job_1705510300565_0005\n",
      "  Job job_1705510300565_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=834321\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=376\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=28\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4918272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1925120\n",
      "\t\tTotal time spent by all map tasks (ms)=4803\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4803\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1880\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1880\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4803\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1880\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1420\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tInput split bytes=284\n",
      "\t\tMap input records=7\n",
      "\t\tMap output bytes=63\n",
      "\t\tMap output materialized bytes=89\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=343412736\n",
      "\t\tPeak Map Virtual memory (bytes)=2549915648\n",
      "\t\tPeak Reduce Physical memory (bytes)=202092544\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2554277888\n",
      "\t\tPhysical memory (bytes) snapshot=885264384\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=89\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=891813888\n",
      "\t\tVirtual memory (bytes) snapshot=7654047744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758/output...\n",
      "\"ana\"\t10\n",
      "\"luis\"\t4\n",
      "\"pedro\"\t8\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/notas.root.20240117.173229.083758...\n",
      "Removing temp directory /tmp/notas.root.20240117.173229.083758...\n"
     ]
    }
   ],
   "source": [
    "! python3 notas.py -r hadoop notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Usando un proceso MapReduce muestra las 10 palabras más utilizadas en `El Quijote`.\n",
    "\n",
    "Lo primero será descargar El Quijote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-07 23:29:34--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M   761KB/s    in 2.9s    \n",
      "\n",
      "2024-01-07 23:29:38 (761 KB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O '2000-0.txt' https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la primera práctica, eliminamos aquellas líneas que son metadata y no forman parte de la obra. Sobrescribimos el fichero sin esas líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing palabrasQuijote.py\n"
     ]
    }
   ],
   "source": [
    "with open('2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "book = lines[head:-tail]\n",
    "\n",
    "with open('2000-0.txt', 'w') as f:\n",
    "    for line in book:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debería ser el mismo que el que obtuvimos en la primera práctica.\n",
    "\n",
    "![solución 2](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting palabrasQuijote.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile palabrasQuijote.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MostUsedWords(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_words)\n",
    "        ]\n",
    "\n",
    "    # Separa cada línea en palabras y las envía al reducer con un conteo de 1.\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    # Suma todos los conteos para cada palabra.\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # Toma todas las palabras y sus conteos, los ordena y devuelve las 10 palabras más frecuentes.\n",
    "    def reducer_find_top_words(self, _, word_count_pairs):\n",
    "        top_words = sorted(word_count_pairs, reverse=True)[:10]\n",
    "        for count, word in top_words:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostUsedWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x palabrasQuijote.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/palabrasQuijote.root.20240117.173636.260181\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2934571464954495916/] [] /tmp/streamjob1139812660036353079.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0007\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0007/\n",
      "  Running job: job_1705510300565_0007\n",
      "  Job job_1705510300565_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2227135\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=527951\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4585617\n",
      "\t\tFILE: Number of bytes written=10005884\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2227441\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=527951\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7640064\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3682304\n",
      "\t\tTotal time spent by all map tasks (ms)=7461\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7461\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3596\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3596\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7461\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3596\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3490\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=218\n",
      "\t\tInput split bytes=306\n",
      "\t\tMap input records=38062\n",
      "\t\tMap output bytes=3811615\n",
      "\t\tMap output materialized bytes=4585623\n",
      "\t\tMap output records=386998\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=342708224\n",
      "\t\tPeak Map Virtual memory (bytes)=2549731328\n",
      "\t\tPeak Reduce Physical memory (bytes)=198610944\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553376768\n",
      "\t\tPhysical memory (bytes) snapshot=849338368\n",
      "\t\tReduce input groups=23837\n",
      "\t\tReduce input records=386998\n",
      "\t\tReduce output records=23837\n",
      "\t\tReduce shuffle bytes=4585623\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=773996\n",
      "\t\tTotal committed heap usage (bytes)=860356608\n",
      "\t\tVirtual memory (bytes) snapshot=7647924224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar935398888758465113/] [] /tmp/streamjob8022755616211004440.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0008\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0008\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0008\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0008/\n",
      "  Running job: job_1705510300565_0008\n",
      "  Job job_1705510300565_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=532047\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=104\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=575631\n",
      "\t\tFILE: Number of bytes written=1985630\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=532375\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=104\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4273152\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2249728\n",
      "\t\tTotal time spent by all map tasks (ms)=4173\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4173\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2197\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2197\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4173\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2197\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1920\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=191\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=23837\n",
      "\t\tMap output bytes=527951\n",
      "\t\tMap output materialized bytes=575637\n",
      "\t\tMap output records=23837\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=338903040\n",
      "\t\tPeak Map Virtual memory (bytes)=2556891136\n",
      "\t\tPeak Reduce Physical memory (bytes)=204029952\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553331712\n",
      "\t\tPhysical memory (bytes) snapshot=852463616\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=23837\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=575637\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=47674\n",
      "\t\tTotal committed heap usage (bytes)=860356608\n",
      "\t\tVirtual memory (bytes) snapshot=7658295296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181/output...\n",
      "\"que\"\t20769\n",
      "\"de\"\t18410\n",
      "\"y\"\t18266\n",
      "\"la\"\t10491\n",
      "\"a\"\t9932\n",
      "\"en\"\t8281\n",
      "\"el\"\t8265\n",
      "\"no\"\t6344\n",
      "\"los\"\t4769\n",
      "\"se\"\t4752\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/palabrasQuijote.root.20240117.173636.260181...\n",
      "Removing temp directory /tmp/palabrasQuijote.root.20240117.173636.260181...\n"
     ]
    }
   ],
   "source": [
    "! python3 palabrasQuijote.py -r hadoop 2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Muestra la clasificación de temporada 2021/2022 de La Liga pero únicamente de los puntos obtenidos como visitante.\n",
    "\n",
    "En [esta Web](https://resultados.as.com/resultados/futbol/primera/2021_2022/clasificacion/) puedes consultar cuántos puntos obtuvo cada equipo fuera de casa.\n",
    "\n",
    "Empezamos descargando el fichero de resultados de la temporada 2021/2022 y renombrándolo a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-07 23:47:25--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K   722KB/s    in 0.2s    \n",
      "\n",
      "2024-01-07 23:47:26 (722 KB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera este resultado:\n",
    "\n",
    "![solución 3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificacion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificacion.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "\n",
    "class VisitorPointsMRJob(MRJob):\n",
    "\n",
    "    # Extrae el equipo visitante y los puntos obtenidos de cada línea (representando un partido) del archivo CSV.\n",
    "    def mapper(self, _, line):\n",
    "         # Convertir la línea en un diccionario usando csv.DictReader\n",
    "        row = dict(zip(['Div', 'Date', 'Time','HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR'], next(csv.reader([line]))))\n",
    "\n",
    "        visitor_team = row['AwayTeam']\n",
    "        points = 0\n",
    "        if row['FTR'] == 'A':\n",
    "            points = 3\n",
    "        elif row['FTR'] == 'D':\n",
    "            points = 1\n",
    "\n",
    "        yield visitor_team, points\n",
    "\n",
    "    # Suma los puntos de cada equipo visitante.\n",
    "    def reducer(self, team, points):\n",
    "        yield team, sum(points)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    VisitorPointsMRJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x clasificacion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/clasificacion.root.20240117.180628.915861\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4181290725929563182/] [] /tmp/streamjob6530032604275923716.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705510300565_0014\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705510300565_0014\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705510300565_0014\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705510300565_0014/\n",
      "  Running job: job_1705510300565_0014\n",
      "  Job job_1705510300565_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705510300565_0014 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=281\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5531\n",
      "\t\tFILE: Number of bytes written=845604\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176580\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=281\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3941376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1817600\n",
      "\t\tTotal time spent by all map tasks (ms)=3849\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3849\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1775\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1775\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3849\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1775\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=310\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=4763\n",
      "\t\tMap output materialized bytes=5537\n",
      "\t\tMap output records=381\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=338718720\n",
      "\t\tPeak Map Virtual memory (bytes)=2548281344\n",
      "\t\tPeak Reduce Physical memory (bytes)=196759552\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553225216\n",
      "\t\tPhysical memory (bytes) snapshot=835440640\n",
      "\t\tReduce input groups=21\n",
      "\t\tReduce input records=381\n",
      "\t\tReduce output records=21\n",
      "\t\tReduce shuffle bytes=5537\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=762\n",
      "\t\tTotal committed heap usage (bytes)=856162304\n",
      "\t\tVirtual memory (bytes) snapshot=7646322688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861/output...\n",
      "\"Alaves\"\t6\n",
      "\"Ath Bilbao\"\t21\n",
      "\"Ath Madrid\"\t30\n",
      "\"AwayTeam\"\t0\n",
      "\"Barcelona\"\t35\n",
      "\"Betis\"\t33\n",
      "\"Cadiz\"\t21\n",
      "\"Celta\"\t21\n",
      "\"Elche\"\t15\n",
      "\"Espanol\"\t9\n",
      "\"Getafe\"\t11\n",
      "\"Granada\"\t16\n",
      "\"Levante\"\t13\n",
      "\"Mallorca\"\t12\n",
      "\"Osasuna\"\t25\n",
      "\"Real Madrid\"\t42\n",
      "\"Sevilla\"\t28\n",
      "\"Sociedad\"\t27\n",
      "\"Valencia\"\t22\n",
      "\"Vallecano\"\t13\n",
      "\"Villarreal\"\t23\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/clasificacion.root.20240117.180628.915861...\n",
      "Removing temp directory /tmp/clasificacion.root.20240117.180628.915861...\n"
     ]
    }
   ],
   "source": [
    "! python3 clasificacion.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Muestra la diferencia de goles entre el equipo que más goles ha marcado y el que menos goles ha marcado en la temporada 2021/2022 de La Liga.\n",
    "\n",
    "Se espera que el proceso MapReuce produzca una salida similar a la siguiente:\n",
    "\n",
    "![solución 4](./img/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting goaldifference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile goaldifference.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "\n",
    "class GoalDifferenceMRJob(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_find_goal_difference)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('Div'):\n",
    "            return\n",
    "        \n",
    "        # Ignorar líneas mal formateadas o la cabecera del CSV\n",
    "        try:\n",
    "            row = list(csv.reader([line]))[0]\n",
    "            if len(row) < 7:\n",
    "                return\n",
    "\n",
    "            home_team = row[3]\n",
    "            away_team = row[4]\n",
    "            home_goals = int(row[5])\n",
    "            away_goals = int(row[6])\n",
    "\n",
    "            yield home_team, home_goals\n",
    "            yield away_team, away_goals\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Saltar líneas que no pueden ser procesadas\n",
    "\n",
    "    def reducer(self, team, goals):\n",
    "        total_goals = sum(goals)\n",
    "        yield None, (team, total_goals)\n",
    "\n",
    "    def reducer_find_goal_difference(self, _, team_goals):\n",
    "        min_goals = float('inf')\n",
    "        max_goals = 0\n",
    "        team_min_goals = None\n",
    "        team_max_goals = None\n",
    "\n",
    "        for team, goals in team_goals:\n",
    "            if goals < min_goals:\n",
    "                min_goals = goals\n",
    "                team_min_goals = team\n",
    "            if goals > max_goals:\n",
    "                max_goals = goals\n",
    "                team_max_goals = team\n",
    "        \n",
    "        # Verificar si se encontraron equipos válidos\n",
    "        yield \"Equipo con mas goles\", (team_max_goals, max_goals)\n",
    "        yield \"Equipo con menos goles\", (team_min_goals, min_goals)\n",
    "        yield \"Diferencia de goles\", max_goals - min_goals\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    GoalDifferenceMRJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x goaldifference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/goaldifference.root.20240119.190711.723555\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8299926449389177728/] [] /tmp/streamjob7856076941581919361.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705599139436_0009\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705599139436_0009\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705599139436_0009\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705599139436_0009/\n",
      "  Running job: job_1705599139436_0009\n",
      "  Job job_1705599139436_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705599139436_0009 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11026\n",
      "\t\tFILE: Number of bytes written=856672\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176582\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8710144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1994752\n",
      "\t\tTotal time spent by all map tasks (ms)=8506\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8506\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1948\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1948\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8506\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1948\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2440\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=515\n",
      "\t\tInput split bytes=312\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=9500\n",
      "\t\tMap output materialized bytes=11032\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=345309184\n",
      "\t\tPeak Map Virtual memory (bytes)=2555392000\n",
      "\t\tPeak Reduce Physical memory (bytes)=203272192\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553360384\n",
      "\t\tPhysical memory (bytes) snapshot=886329344\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=760\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=11032\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1520\n",
      "\t\tTotal committed heap usage (bytes)=871890944\n",
      "\t\tVirtual memory (bytes) snapshot=7657803776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2332970028639248326/] [] /tmp/streamjob7683246900457791897.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705599139436_0010\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705599139436_0010\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705599139436_0010\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705599139436_0010/\n",
      "  Running job: job_1705599139436_0010\n",
      "  Job job_1705599139436_0010 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705599139436_0010 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=108\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835281\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=971\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=108\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4102144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1862656\n",
      "\t\tTotal time spent by all map tasks (ms)=4006\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4006\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1819\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1819\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4006\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1819\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1120\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=182\n",
      "\t\tInput split bytes=326\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=342208512\n",
      "\t\tPeak Map Virtual memory (bytes)=2550341632\n",
      "\t\tPeak Reduce Physical memory (bytes)=196485120\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2553192448\n",
      "\t\tPhysical memory (bytes) snapshot=874229760\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=876609536\n",
      "\t\tVirtual memory (bytes) snapshot=7651323904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555/output...\n",
      "\"Equipo con mas goles\"\t[\"Real Madrid\", 80]\n",
      "\"Equipo con menos goles\"\t[\"Alaves\", 31]\n",
      "\"Diferencia de goles\"\t49\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/goaldifference.root.20240119.190711.723555...\n",
      "Removing temp directory /tmp/goaldifference.root.20240119.190711.723555...\n"
     ]
    }
   ],
   "source": [
    "! python3 goaldifference.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Calcula la racha de los últimos cinco partidos de cada equipo en la clasificación final de La Liga en la temporada 2021/2022.\n",
    "\n",
    "[Observa](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) que las últimas columnas de la clasificación muestran cuál ha sido el resultado de los últimos 5 partidos de cada equipo.\n",
    "\n",
    "![clasificacion](./img/clasificacion.png)\n",
    "\n",
    "Se trata de que muestres la clasificación final junto con los resultados de los últimos 5 partidos. Este ejercicio es un poco más difícil y laborioso que los otros. Si usas `mrjob` probablemente te sea útil utilizar [ordenación secundaria por valor](https://mrjob.readthedocs.io/en/latest/job.html#secondary-sort), aunque también se puede resolver sin hacer uso de ella.\n",
    "\n",
    "Se espera este resultado:\n",
    "\n",
    "![solución 5](./img/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clasificationMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clasificationMR.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "#!/usr/bin/env python3\n",
    "from mrjob.job import MRJob\n",
    "from collections import deque\n",
    "\n",
    "class TeamPointsMRJob(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Dividir la línea y extraer los campos necesarios\n",
    "        row = line.split(',')\n",
    "        if row[0] == 'Div':  # Saltar la cabecera\n",
    "            return\n",
    "        try:\n",
    "            home_team = row[3]\n",
    "            away_team = row[4]\n",
    "            home_goals = int(row[5])\n",
    "            away_goals = int(row[6])\n",
    "            result = 'H' if home_goals > away_goals else 'D' if home_goals == away_goals else 'A'\n",
    "\n",
    "            # Emitir puntos y resultado para el equipo local y visitante\n",
    "            if result == 'H':\n",
    "                yield home_team, ('W', 3)\n",
    "                yield away_team, ('L', 0)\n",
    "            elif result == 'D':\n",
    "                yield home_team, ('D', 1)\n",
    "                yield away_team, ('D', 1)\n",
    "            elif result == 'A':\n",
    "                yield away_team, ('W', 3)\n",
    "                yield home_team, ('L', 0)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Saltar líneas mal formadas\n",
    "\n",
    "    def reducer(self, team, values):\n",
    "        total_points = 0\n",
    "        last_five = deque(maxlen=5)\n",
    "        for result, points in values:\n",
    "            total_points += points\n",
    "            # Convertir 'W', 'D', 'L' a valores numéricos y añadir a last_five\n",
    "            numeric_result = 3 if result == 'W' else 1 if result == 'D' else 0\n",
    "            last_five.append(numeric_result)\n",
    "        yield team, (total_points, list(last_five))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TeamPointsMRJob.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x clasificationMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/clasificationMR.root.20240119.190845.765610\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8071806759620557945/] [] /tmp/streamjob6371129225352331386.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1705599139436_0011\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1705599139436_0011\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1705599139436_0011\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1705599139436_0011/\n",
      "  Running job: job_1705599139436_0011\n",
      "  Job job_1705599139436_0011 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1705599139436_0011 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=650\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16346\n",
      "\t\tFILE: Number of bytes written=867327\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176584\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=650\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4206592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1914880\n",
      "\t\tTotal time spent by all map tasks (ms)=4108\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4108\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1870\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1870\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4108\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1870\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=141\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=14820\n",
      "\t\tMap output materialized bytes=16352\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=337780736\n",
      "\t\tPeak Map Virtual memory (bytes)=2551169024\n",
      "\t\tPeak Reduce Physical memory (bytes)=205090816\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2559516672\n",
      "\t\tPhysical memory (bytes) snapshot=849145856\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=760\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=16352\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1520\n",
      "\t\tTotal committed heap usage (bytes)=853016576\n",
      "\t\tVirtual memory (bytes) snapshot=7661076480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610/output...\n",
      "\"Alaves\"\t[31, [0, 0, 0, 0, 1]]\n",
      "\"Ath Bilbao\"\t[55, [0, 3, 1, 3, 3]]\n",
      "\"Ath Madrid\"\t[71, [0, 3, 0, 0, 3]]\n",
      "\"Barcelona\"\t[73, [3, 3, 0, 0, 1]]\n",
      "\"Betis\"\t[65, [3, 0, 3, 1, 0]]\n",
      "\"Cadiz\"\t[39, [1, 0, 1, 3, 0]]\n",
      "\"Celta\"\t[46, [0, 0, 3, 3, 0]]\n",
      "\"Elche\"\t[42, [3, 0, 3, 0, 0]]\n",
      "\"Espanol\"\t[42, [0, 3, 3, 0, 1]]\n",
      "\"Getafe\"\t[39, [0, 1, 1, 0, 0]]\n",
      "\"Granada\"\t[38, [0, 0, 1, 1, 1]]\n",
      "\"Levante\"\t[35, [3, 0, 0, 3, 0]]\n",
      "\"Mallorca\"\t[39, [0, 1, 0, 3, 1]]\n",
      "\"Osasuna\"\t[47, [0, 0, 1, 3, 1]]\n",
      "\"Real Madrid\"\t[86, [3, 1, 3, 3, 3]]\n",
      "\"Sevilla\"\t[70, [3, 1, 1, 1, 3]]\n",
      "\"Sociedad\"\t[62, [1, 3, 3, 3, 0]]\n",
      "\"Valencia\"\t[48, [0, 1, 0, 3, 0]]\n",
      "\"Vallecano\"\t[42, [3, 0, 3, 1, 1]]\n",
      "\"Villarreal\"\t[59, [0, 0, 3, 3, 3]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/clasificationMR.root.20240119.190845.765610...\n",
      "Removing temp directory /tmp/clasificationMR.root.20240119.190845.765610...\n"
     ]
    }
   ],
   "source": [
    "! python3 clasificationMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
