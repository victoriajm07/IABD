# Ecosistema Hadoop

En la unidad anterior se explicó la base de Hadoop, que se conoce como Hadoop Core, y que está formado por:

- HDFS, que es la capa de almacenamiento.
- YARN, que es el gestor de los procesos que se ejecutan en el clúster.
- MapReduce, que es un modelo de programación para desarrollar tareas de procesamiento de datos.

Sólo con estas piezas, Hadoop no ofrece mucha funcionalidad, ya que para cualquier análisis se requiere que un equipo de expertos en Big Data programe procesos en MapReduce, que no son sencillos de implementar, y peor aún, más difíciles de mantener en caso de que los datos cambien, o se quiera obtener algún dato adicional como resultado.

Además, tanto las ingestas como cualquier operación con la plataforma, se realiza mediante consolas y comandos difíciles de utilizar para cualquier persona que no sea un experto en tecnología.

Para corregir estos problemas y hacer la plataforma mucho más usable, surgen los componentes del ecosistema Hadoop. Estos componentes ofrecen funcionalidades específicas con las que Hadoop puede convertirse en una plataforma de datos global para toda la empresa, y con las que no sólo los expertos técnicos pueden utilizarla.

En esta unidad vamos a conocer todas estas funcionalidades:

- En primer lugar, conoceremos las funcionalidades asociadas con el almacenamiento y el procesamiento de datos.
- A continuación, conoceremos las funcionalidades para poder ingestar datos o automatizar procesos de trabajo.
- Después conoceremos otros interfaces que abren Hadoop a un uso más amplio, sin requerir grandes conocimientos técnicos.
- Por último, veremos las herramientas para procesamiento en streaming o tiempo real.

![image](https://github.com/victoriajm07/IABD/assets/122750285/b22b5922-daa8-4d35-bfca-2559906d28bd)
![image](https://github.com/victoriajm07/IABD/assets/122750285/255c904a-9e3b-4de0-99ce-3e2a4ff9e1b6)
